***

# ClearThink: Adaptive Attention-Based Pruning for Reasoning Models

![Python](https://img.shields.io/badge/Python-3.10%2B-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-orange)
![Model](https://img.shields.io/badge/Model-DeepSeek--R1--Distill-gray)

**ClearThink** is an inference-time optimization framework designed to mitigate "Overthinking" in System 2 reasoning models. By surgically removing redundant Chain-of-Thought (CoT) tokens during generation, it aims to reduce context window usage without degrading accuracy on complex mathematical tasks.

This project implements and extends concepts from **[Think Clearly (Choi et al., 2025)](https://arxiv.org/abs/2507.08806)** and **[TRAAC (Singh et al., 2025)](https://arxiv.org/abs/2510.01581)**, applying them to the **AIME 2024** benchmark using `DeepSeek-R1-Distill-Qwen-7B`.

---

## ðŸ“‚ Repository Structure & Roadmap

This project was built in a modular, 12-stage pipeline. The files correspond to specific components of the inference engine:

### Phase 1: Infrastructure & Baseline
*   **`hour1.py` (Baseline):** Runs the standard model without intervention to establish ground-truth accuracy (60% on short context) and token usage (~6k tokens/problem).
*   **`hour2.py` (Segmentation):** Implements robust reasoning step detection using Token ID mapping (detecting "Wait", "Therefore", etc.) rather than brittle string matching.
*   **`hour3.py` (Generation Loop):** A custom manual decoding loop that replaces `model.generate()`, allowing for real-time intervention at step boundaries.

### Phase 2: The Core "Think Clearly" Engine
*   **`hour4.py` (The Probe):** Implements the **Stateless Forked Probe**. It pauses generation, injects a "Time is up" prompt, and measures the attention of the `</think>` token to the history *without* polluting the main KV cache.
*   **`hour5.py` (Scoring):** Aggregates raw attention weights into "Chunk Importance Scores," identifying which reasoning steps the model considers "fluff."
*   **`hour6.py` (The Pruner):** **Key Innovation.** Implements the **"Context Refresh"** strategy. Instead of slicing the KV cache (which breaks RoPE embeddings), this module reconstructs the prompt without the low-score chunks, flushes the cache, and re-feeds the optimized context.

### Phase 3: Experiments & Analysis
*   **`hour7.py` - `hour8.py`:** Integration and stability optimization (handling OOM errors via windowed probing).
*   **`hour9.py` (Experiment A):** **Conservative Pruning** (Threshold: Bottom 25%).
*   **`hour10.py` (Experiment B):** **Balanced Pruning** (Threshold: Bottom 40%).
*   **`hour11.py` (Experiment C):** **Aggressive Pruning** (Threshold: Bottom 60%).
*   **`hour12.py` (Analysis):** Parses logs, validates AIME answers using rigorous Regex, and generates the final comparison metrics.

---

## ðŸ§  Key Technical Innovations

### 1. The "Context Refresh" Strategy
Traditional KV-cache pruning fails on modern models (like Llama/Qwen) because of **Rotary Positional Embeddings (RoPE)**. If tokens 10-20 are deleted, token 21 retains its "position 21" rotation but shifts to index 11, destroying the geometric attention mechanism.

**Our Solution:**
1. Identify "Redundant Chunks" via Attention Probing.
2. Reconstruct the text string *excluding* those chunks.
3. **Flush the KV Cache (`None`)** completely.
4. Re-tokenize and re-feed the cleaned text.
*Result: Seamless coherence maintenance with 100% mathematical integrity.*

### 2. Forked Attention Probing
To measure the importance of a reasoning step, we fork the forward pass. We append a summarization probe to a *copy* of the input. This allows us to inspect the model's "meta-cognition" (what it thinks is important) without the probe itself becoming part of the generation history.

---

## ðŸ“Š Experimental Results (AIME 2024)

We evaluated three difficulty-adaptive strategies on a subset of the AIME 2024 dataset.

| Strategy | File | Pruning Threshold | Accuracy | Avg Reduction | Observations |
| :--- | :--- | :--- | :--- | :--- | :--- |
| **Conservative** | `hour9.py` | Bottom 25% | **40.0%** | 19.7% | **Optimal.** Removed arithmetic scaffolding while preserving logic. |
| **Balanced** | `hour10.py` | Bottom 40% | 30.0% | 21.2% | Diminishing returns. Accuracy dropped without significant space savings. |
| **Aggressive** | `hour11.py` | Bottom 60% | 20.0% | 22.2% | **Failure Mode.** Caused "Regeneration Loops." |

---

## ðŸ’¡ Key Insights & Discussion

### 1. The "Regeneration Loop" Phenomenon
In **Experiment C (Aggressive)**, we observed a counter-intuitive result: pruning *more* aggressively resulted in *truncation* failures.
*   **Observation:** When a critical logical step was pruned, the model would often "realize" the context was missing and **regenerate the exact same step** immediately after the refresh.
*   **Implication:** There is a "floor" to compression. System 2 reasoning requires a minimum scaffolding structure. Below this, the model cycles indefinitely, trying to rebuild its lost thought process.

### 2. Attention is Polarized
Comparing Experiment A (Conservative) and B (Balanced) revealed that increasing the threshold from 25% to 40% only increased reduction by ~1.5%.
*   **Insight:** Reasoning steps tend to be binary in utility. They are either **Critical** (High Attention) or **Noise** (Very Low Attention). There is very little "middle ground," suggesting that conservative pruning is sufficient to capture the majority of efficiency gains.

### 3. Math requires "Hard Mode"
Our results align with the **TRAAC** hypothesis: difficult problems (AIME) require *lower* compression rates (Conservative). Aggressive strategies that work for simple QA (GSM8K) fail here because "underthinking" breaks the multi-step dependency chain required for Number Theory and Algebra.

---

## ðŸ’» Usage

To replicate the optimal result (Conservative Mode):

1.  **Install Dependencies:**
    ```bash
    pip install torch transformers accelerate datasets numpy pandas
    ```

2.  **Run the Experiment:**
    ```bash
    python hour9.py
    ```

3.  **View Results:**
    The script outputs `results_exp_a_windowed.json`. Run `hour12.py` to convert this into a CSV report.

---

## ðŸ“œ References

1.  **Think Clearly: Improving Reasoning via Redundant Token Pruning** - *Choi et al. (2025)*
2.  **Think Right: Learning to Mitigate Under-Over Thinking (TRAAC)** - *Singh et al. (2025)*
